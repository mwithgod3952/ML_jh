{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "소논문리뷰 _ Tslearn, A Machine Learning Toolkit for Time Series Data Romain Tavenard, Johann Faouzi, Gilles Vandewiele, Felix Divo, Guillau",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "y5UYsE-fK-0O"
      ],
      "authorship_tag": "ABX9TyO+IYuqlXQnFZB9glthD78i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwithgod3952/Machine_Learning_Theory_jh/blob/master/%EC%86%8C%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0___Tslearn%2C_A_Machine_Learning_Toolkit_for_Time_Series_Data_Romain_Tavenard%2C_Johann_Faouzi%2C_Gilles_Vandewiele%2C_Felix_Divo%2C_Guillau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5UYsE-fK-0O"
      },
      "source": [
        "##### 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxwTr5dGwA3S"
      },
      "source": [
        "!python -m pip install tslearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BckgEUiDutY9"
      },
      "source": [
        "#### <font color = 'blue'>***사전학습***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_A3tfIc_ixV"
      },
      "source": [
        "    1. Cross_Validation(교차검증)\n",
        "        > 교차검증은 train_set을 train set + validation_set으로 분리한 뒤 validation set을 사용해 검증하는 방식이다. \n",
        "\n",
        "            만약, train_set을 통해 특정 모델을 활용한 학습과정을 거친 뒤, 일괄적으로 동일한 test_set을 통한 반복적 검증을 한다면, \n",
        "            특정 Feature Engineering 등을 통해 일괄적으로 모델의 성능은 올릴 수 있겠으나, 과적합(Overfitting)문제가 발생할 수 있으며, \n",
        "            추 후 새로운 데이터가 유입될 시 올바른 결과를 낼 수 없다.\n",
        "<font color = 'red'>***(Regression의 경우 MSE가, Classification의 경우 ER(Error Rate)현저히 다르게 나타날 수 있다는 것이다.)***</font>\n",
        "\n",
        "    1-1 교차검증의 장점 \n",
        "\n",
        "        - 모든 데이터셋을 훈련에 활용할 수 있다.\n",
        "        - 정확도를 향상시킬 수 있다.\n",
        "        - 데이터 부족으로 인한 underfitting을 방지할 수 있다.\n",
        "        - 평가에 사용되는 데이터 편중을 막을 수 있다.\n",
        "        - 평가 결과에 따라 좀 더 일반화된 모델을 만들 수 있다.\n",
        "\n",
        "    1-2 교차검증의 단점\n",
        "\n",
        "        - Iteration 횟수가 많기 때문에, 모델 훈련/평가 시간이 오래 걸린다.\n",
        "\n",
        "    1-3. 교차검증의 종류\n",
        "        - K-Fold Cross Validation ( k-겹 교차 검증 )\n",
        "            * cross-validation-splitter\n",
        "            * Leave-one-out-cross-validation( LOOCV )\n",
        "            * shuffle-split-cross-validation\n",
        "        - Stratified K-Fold Cross Validation ( 계층별 k-겹 교차 검증 )    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBj1HhDjtkTR"
      },
      "source": [
        "    2 Grid-Search\n",
        "        > 매개변수를 튜닝하여 일반화 성능을 개선\n",
        "        > 가능한 조합들을 시도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkrDTsTNv_xq"
      },
      "source": [
        "    - 간단한 그리드서치 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqY0N5BRwEH7"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccb5fjdmwGF4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
        "print(\"훈련 세트의 크기: {} 테스트 세트의 크기: {}\".format(X_train.shape[0], X_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0osrRmVcqtHQ"
      },
      "source": [
        "best_score = 0\n",
        "gm = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "for gamma in gm:\n",
        "    for C in gm:\n",
        "        svm = SVC(gamma=gamma, C=C)\n",
        "        svm.fit(X_train, y_train)\n",
        "        score = svm.score(X_test, y_test)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_parameters = {'C' : C, 'gamma' : gamma}\n",
        "\n",
        "print(\"최고 점수: {:.2f}\".format(best_score))\n",
        "print(\"최고 매개변수:\", best_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0PIGpOHuO89"
      },
      "source": [
        "#### <font color = 'blue'>***study_tslearn***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjVXjCKPt0HU"
      },
      "source": [
        "from tslearn.utils import to_time_series_dataset\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COz0XiGyt2sS"
      },
      "source": [
        "my_first_time_series = [1, 3, 4, 2]\n",
        "my_second_time_series = [1, 2, 4, 2, 1]\n",
        "# array 를 temporal data로 변경 및 formatted_dataset은 2개의 time series를 내포하고 있음.\n",
        "formatted_dataset = to_time_series_dataset([my_first_time_series, my_second_time_series])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1YlDR_Wt5Uk"
      },
      "source": [
        "print(formatted_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWWskkmbt71H"
      },
      "source": [
        "print(formatted_dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZA53R2mt_ZY"
      },
      "source": [
        "***represented through a three dimenssional numpy array of shape (n,T,d)***\n",
        "\n",
        "  - n : number of time seriese\n",
        "  - T : length\n",
        "  - d : dimentionality\n",
        "\n",
        "    * 즉 2개의 Temporal data가 5개의 sample 길이 및 1개의 차원으로 구성되어 져있다고 위 데이터셋을 설명할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yP8naOSR21p"
      },
      "source": [
        "***Note***\n",
        "\n",
        "    - scikit-learn’s pipelines and model-selection tools can be used in conjunction with tslearn transformers and estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6c3cf_jR26k"
      },
      "source": [
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp4opWXrxUOi"
      },
      "source": [
        "***코드 예시***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ohzg1TRTUN-"
      },
      "source": [
        "knn = KNeighborsTimeSeriesClassifier(metric=\"dtw\")\n",
        "p_grid = {\"n_neighbors\" : [1, 5]}\n",
        "cv = KFold(n_splits=2, shuffle=True, random_state=0)\n",
        "clf = GridSearchCV(estimator=knn, param_grid=p_grid, cv=cv)\n",
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6WPjKMJwDWD"
      },
      "source": [
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.datasets import CachedDatasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUV1VQ3vzbLM"
      },
      "source": [
        "X_train = CachedDatasets().load_dataset('Trace')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "175BHJvwziIF"
      },
      "source": [
        "euclidean_params = {'metric':'euclidean'}\n",
        "dba_params = {'metric':'dtw'}\n",
        "sdtw_params = {'metric':'softdtw', 'metric_params':{'gamma':.01}}\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for params in (euclidean_params, dba_params, sdtw_params):\n",
        "    km = TimeSeriesKMeans(n_clusters=3, random_state=0, **params)\n",
        "    y_preds.append(km.fit_predict(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi-6qSzlGIve"
      },
      "source": [
        "***Time series clustering with kmeans***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq2IpVLcGIMY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9YzWy7JFAM"
      },
      "source": [
        "#  make df\n",
        "col_name = ['X_train', 'y_train', 'X_test', 'y_test']\n",
        "df = pd.DataFrame()\n",
        "idx = 0\n",
        "for name in col_name:\n",
        "    df[name] = CachedDatasets().load_dataset(\"Trace\")[idx].tolist()\n",
        "    idx  += 1\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUHbjZgtGr6Q"
      },
      "source": [
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
        "X_train = X_train[y_train < 4]  \n",
        "numpy.random.shuffle(X_train)\n",
        "\n",
        "X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train[:50])\n",
        "X_train = TimeSeriesResampler(sz=40).fit_transform(X_train)\n",
        "sz = X_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGuJsBhCF40M"
      },
      "source": [
        "print(\"Euclidean k-means\")\n",
        "km = TimeSeriesKMeans(n_clusters=3, verbose=True, random_state=seed)\n",
        "y_pred = km.fit_predict(X_train)\n",
        "\n",
        "plt.figure(figsize = (15, 10))\n",
        "for yi in range(3):\n",
        "    plt.subplot(3, 3, yi + 1)\n",
        "    for xx in X_train[y_pred == yi]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"Euclidean k-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly67yFL4JsSS"
      },
      "source": [
        "dba_km = TimeSeriesKMeans(n_clusters=3,\n",
        "                          n_init=2,\n",
        "                          metric=\"dtw\",\n",
        "                          verbose=True,\n",
        "                          max_iter_barycenter=10,\n",
        "                          random_state=seed)\n",
        "y_pred = dba_km.fit_predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egvx3DiYKTvi"
      },
      "source": [
        "plt.figure(figsize = (15, 10))\n",
        "for yi in range(3):\n",
        "    plt.subplot(3, 3, 4 + yi)\n",
        "    for xx in X_train[y_pred == yi]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(dba_km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"DBA $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJjuFptNKGM6"
      },
      "source": [
        "# Soft-DTW-k-means\n",
        "print(\"Soft-DTW k-means\")\n",
        "sdtw_km = TimeSeriesKMeans(n_clusters=3,\n",
        "                           metric=\"softdtw\",\n",
        "                           metric_params={\"gamma\": .01},\n",
        "                           verbose=True,\n",
        "                           random_state=seed)\n",
        "y_pred = sdtw_km.fit_predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKoetADeKg6a"
      },
      "source": [
        "plt.figure(figsize = (15, 10))\n",
        "for yi in range(3):\n",
        "    plt.subplot(3, 3, 7 + yi)\n",
        "    for xx in X_train[y_pred == yi]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(sdtw_km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"Soft-DTW $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}